#!/usr/bin/env python3
"""
cnn_lstm_model.py
CNN-LSTM ìœµí•© ëª¨ë¸ ì •ì˜ ë° êµ¬ì¶• ëª¨ë“ˆ
"""

import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import (
    Input, Dense, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D,
    Dropout, BatchNormalization, Concatenate, Reshape, Flatten,
    Bidirectional, MultiHeadAttention, LayerNormalization, Add
)
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import (
    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,
    TensorBoard, CSVLogger
)
import numpy as np

class CNNLSTMBuilder:
    """CNN-LSTM ìœµí•© ëª¨ë¸ ë¹Œë” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self.model_config = {}
        
    def build_basic_cnn_lstm(self, input_shape, 
                            cnn_filters=[64, 32], 
                            lstm_units=[64, 32],
                            dense_units=[64, 32],
                            dropout_rate=0.2):
        """
        ê¸°ë³¸ CNN-LSTM ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_shape (tuple): ì…ë ¥ í˜•íƒœ (sequence_length, features)
            cnn_filters (list): CNN í•„í„° ìˆ˜ ë¦¬ìŠ¤íŠ¸
            lstm_units (list): LSTM ìœ ë‹› ìˆ˜ ë¦¬ìŠ¤íŠ¸
            dense_units (list): Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜ ë¦¬ìŠ¤íŠ¸
            dropout_rate (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            
        Returns:
            tf.keras.Model: êµ¬ì¶•ëœ ëª¨ë¸
        """
        print("ğŸ§  ê¸°ë³¸ CNN-LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        
        # ì…ë ¥ ë ˆì´ì–´
        inputs = Input(shape=input_shape, name='input_layer')
        
        # CNN ë¸Œëœì¹˜ (íŒ¨í„´ ì¸ì‹)
        cnn_branch = inputs
        for i, filters in enumerate(cnn_filters):
            cnn_branch = Conv1D(
                filters=filters, 
                kernel_size=3, 
                activation='relu', 
                padding='same',
                name=f'conv1d_{i+1}'
            )(cnn_branch)
            cnn_branch = BatchNormalization(name=f'batch_norm_cnn_{i+1}')(cnn_branch)
            cnn_branch = Dropout(dropout_rate, name=f'dropout_cnn_{i+1}')(cnn_branch)
        
        # CNN ì¶œë ¥ì„ ìœ„í•œ Global Max Pooling
        cnn_output = GlobalMaxPooling1D(name='global_maxpool')(cnn_branch)
        
        # LSTM ë¸Œëœì¹˜ (ì‹œê³„ì—´ ì˜ì¡´ì„±)
        lstm_branch = inputs
        for i, units in enumerate(lstm_units[:-1]):
            lstm_branch = LSTM(
                units=units, 
                return_sequences=True,
                dropout=dropout_rate,
                recurrent_dropout=dropout_rate,
                name=f'lstm_{i+1}'
            )(lstm_branch)
        
        # ë§ˆì§€ë§‰ LSTM ë ˆì´ì–´ (return_sequences=False)
        lstm_output = LSTM(
            units=lstm_units[-1],
            dropout=dropout_rate,
            recurrent_dropout=dropout_rate,
            name=f'lstm_{len(lstm_units)}'
        )(lstm_branch)
        
        # ë¸Œëœì¹˜ ìœµí•©
        merged = Concatenate(name='concatenate')([cnn_output, lstm_output])
        
        # ìœµí•© ë ˆì´ì–´
        fusion = merged
        for i, units in enumerate(dense_units):
            fusion = Dense(
                units, 
                activation='relu', 
                name=f'dense_{i+1}'
            )(fusion)
            fusion = Dropout(dropout_rate, name=f'dropout_dense_{i+1}')(fusion)
        
        # ì¶œë ¥ ë ˆì´ì–´
        outputs = Dense(1, activation='linear', name='output')(fusion)
        
        # ëª¨ë¸ ìƒì„±
        model = Model(inputs=inputs, outputs=outputs, name='BasicCNN_LSTM')
        
        self.model_config = {
            'type': 'basic_cnn_lstm',
            'input_shape': input_shape,
            'cnn_filters': cnn_filters,
            'lstm_units': lstm_units,
            'dense_units': dense_units,
            'dropout_rate': dropout_rate
        }
        
        print(f"âœ… ê¸°ë³¸ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        return model
    
    def build_advanced_cnn_lstm(self, input_shape,
                               cnn_filters=[64, 64, 32],
                               lstm_units=[128, 64],
                               dense_units=[128, 64, 32],
                               dropout_rate=0.3,
                               use_attention=True,
                               use_bidirectional=True):
        """
        ê³ ê¸‰ CNN-LSTM ëª¨ë¸ êµ¬ì¶• (Attention, Bidirectional í¬í•¨)
        
        Args:
            input_shape (tuple): ì…ë ¥ í˜•íƒœ
            cnn_filters (list): CNN í•„í„° ìˆ˜
            lstm_units (list): LSTM ìœ ë‹› ìˆ˜
            dense_units (list): Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜
            dropout_rate (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            use_attention (bool): Attention ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© ì—¬ë¶€
            use_bidirectional (bool): Bidirectional LSTM ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            tf.keras.Model: êµ¬ì¶•ëœ ëª¨ë¸
        """
        print("ğŸš€ ê³ ê¸‰ CNN-LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        
        inputs = Input(shape=input_shape, name='input_layer')
        
        # Multi-scale CNN ë¸Œëœì¹˜
        cnn_branches = []
        kernel_sizes = [3, 5, 7]
        
        for i, kernel_size in enumerate(kernel_sizes):
            branch = inputs
            for j, filters in enumerate(cnn_filters):
                branch = Conv1D(
                    filters=filters,
                    kernel_size=kernel_size,
                    activation='relu',
                    padding='same',
                    name=f'conv1d_k{kernel_size}_{j+1}'
                )(branch)
                branch = BatchNormalization(name=f'bn_k{kernel_size}_{j+1}')(branch)
                branch = Dropout(dropout_rate, name=f'dropout_k{kernel_size}_{j+1}')(branch)
            
            # ê° ë¸Œëœì¹˜ì˜ ì¶œë ¥
            branch_output = GlobalMaxPooling1D(name=f'global_pool_k{kernel_size}')(branch)
            cnn_branches.append(branch_output)
        
        # CNN ë¸Œëœì¹˜ ê²°í•©
        if len(cnn_branches) > 1:
            cnn_combined = Concatenate(name='cnn_concat')(cnn_branches)
        else:
            cnn_combined = cnn_branches[0]
        
        # LSTM ë¸Œëœì¹˜
        lstm_branch = inputs
        
        for i, units in enumerate(lstm_units[:-1]):
            if use_bidirectional:
                lstm_branch = Bidirectional(
                    LSTM(
                        units=units//2,  # Bidirectionalì´ë¯€ë¡œ ì ˆë°˜ í¬ê¸° ì‚¬ìš©
                        return_sequences=True,
                        dropout=dropout_rate,
                        recurrent_dropout=dropout_rate
                    ),
                    name=f'bi_lstm_{i+1}'
                )(lstm_branch)
            else:
                lstm_branch = LSTM(
                    units=units,
                    return_sequences=True,
                    dropout=dropout_rate,
                    recurrent_dropout=dropout_rate,
                    name=f'lstm_{i+1}'
                )(lstm_branch)
        
        # ë§ˆì§€ë§‰ LSTM ë ˆì´ì–´
        if use_bidirectional:
            lstm_output = Bidirectional(
                LSTM(
                    units=lstm_units[-1]//2,
                    dropout=dropout_rate,
                    recurrent_dropout=dropout_rate
                ),
                name=f'bi_lstm_{len(lstm_units)}'
            )(lstm_branch)
        else:
            lstm_output = LSTM(
                units=lstm_units[-1],
                dropout=dropout_rate,
                recurrent_dropout=dropout_rate,
                name=f'lstm_{len(lstm_units)}'
            )(lstm_branch)
        
        # Attention ë©”ì»¤ë‹ˆì¦˜ (ì˜µì…˜)
        if use_attention:
            # Self-attentionì„ ìœ„í•œ ì„ì‹œ êµ¬í˜„
            attention_output = Dense(lstm_units[-1], activation='softmax', name='attention_weights')(lstm_output)
            lstm_output = tf.keras.layers.Multiply(name='attention_applied')([lstm_output, attention_output])
        
        # ë¸Œëœì¹˜ ìœµí•©
        merged = Concatenate(name='final_concat')([cnn_combined, lstm_output])
        
        # ìœµí•© ë ˆì´ì–´ë“¤
        fusion = merged
        for i, units in enumerate(dense_units):
            fusion = Dense(units, activation='relu', name=f'fusion_dense_{i+1}')(fusion)
            fusion = BatchNormalization(name=f'fusion_bn_{i+1}')(fusion)
            fusion = Dropout(dropout_rate, name=f'fusion_dropout_{i+1}')(fusion)
        
        # ì¶œë ¥ ë ˆì´ì–´
        outputs = Dense(1, activation='linear', name='output')(fusion)
        
        # ëª¨ë¸ ìƒì„±
        model = Model(inputs=inputs, outputs=outputs, name='AdvancedCNN_LSTM')
        
        self.model_config = {
            'type': 'advanced_cnn_lstm',
            'input_shape': input_shape,
            'cnn_filters': cnn_filters,
            'lstm_units': lstm_units,
            'dense_units': dense_units,
            'dropout_rate': dropout_rate,
            'use_attention': use_attention,
            'use_bidirectional': use_bidirectional
        }
        
        print(f"âœ… ê³ ê¸‰ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        return model
    
    def build_transformer_cnn_lstm(self, input_shape,
                                  cnn_filters=[64, 32],
                                  lstm_units=[64, 32],
                                  transformer_heads=4,
                                  transformer_dim=64,
                                  dense_units=[64, 32],
                                  dropout_rate=0.2):
        """
        Transformer + CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•
        
        Args:
            input_shape (tuple): ì…ë ¥ í˜•íƒœ
            cnn_filters (list): CNN í•„í„° ìˆ˜
            lstm_units (list): LSTM ìœ ë‹› ìˆ˜
            transformer_heads (int): Multi-head Attentionì˜ í—¤ë“œ ìˆ˜
            transformer_dim (int): Transformer ì°¨ì›
            dense_units (list): Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜
            dropout_rate (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            
        Returns:
            tf.keras.Model: êµ¬ì¶•ëœ ëª¨ë¸
        """
        print("ğŸ”® Transformer-CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        
        inputs = Input(shape=input_shape, name='input_layer')
        
        # Transformer ë¸Œëœì¹˜
        transformer_branch = inputs
        
        # Multi-head Self-attention
        attention_output = MultiHeadAttention(
            num_heads=transformer_heads,
            key_dim=transformer_dim,
            name='multi_head_attention'
        )(transformer_branch, transformer_branch)
        
        # Add & Norm
        attention_output = Add(name='add_1')([inputs, attention_output])
        attention_output = LayerNormalization(name='layer_norm_1')(attention_output)
        
        # Feed Forward Network
        ffn = Dense(transformer_dim * 2, activation='relu', name='ffn_1')(attention_output)
        ffn = Dense(input_shape[-1], name='ffn_2')(ffn)
        
        # Add & Norm
        transformer_output = Add(name='add_2')([attention_output, ffn])
        transformer_output = LayerNormalization(name='layer_norm_2')(transformer_output)
        
        # CNN ë¸Œëœì¹˜
        cnn_branch = transformer_output  # Transformer ì¶œë ¥ì„ CNN ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
        for i, filters in enumerate(cnn_filters):
            cnn_branch = Conv1D(
                filters=filters,
                kernel_size=3,
                activation='relu',
                padding='same',
                name=f'conv1d_{i+1}'
            )(cnn_branch)
            cnn_branch = BatchNormalization(name=f'cnn_bn_{i+1}')(cnn_branch)
            cnn_branch = Dropout(dropout_rate, name=f'cnn_dropout_{i+1}')(cnn_branch)
        
        cnn_output = GlobalMaxPooling1D(name='cnn_global_pool')(cnn_branch)
        
        # LSTM ë¸Œëœì¹˜
        lstm_branch = transformer_output
        for i, units in enumerate(lstm_units[:-1]):
            lstm_branch = LSTM(
                units=units,
                return_sequences=True,
                dropout=dropout_rate,
                recurrent_dropout=dropout_rate,
                name=f'lstm_{i+1}'
            )(lstm_branch)
        
        lstm_output = LSTM(
            units=lstm_units[-1],
            dropout=dropout_rate,
            recurrent_dropout=dropout_rate,
            name=f'lstm_{len(lstm_units)}'
        )(lstm_branch)
        
        # ëª¨ë“  ë¸Œëœì¹˜ ìœµí•©
        merged = Concatenate(name='final_merge')([cnn_output, lstm_output])
        
        # ìœµí•© ë ˆì´ì–´
        fusion = merged
        for i, units in enumerate(dense_units):
            fusion = Dense(units, activation='relu', name=f'fusion_{i+1}')(fusion)
            fusion = Dropout(dropout_rate, name=f'fusion_dropout_{i+1}')(fusion)
        
        # ì¶œë ¥
        outputs = Dense(1, activation='linear', name='output')(fusion)
        
        model = Model(inputs=inputs, outputs=outputs, name='TransformerCNN_LSTM')
        
        self.model_config = {
            'type': 'transformer_cnn_lstm',
            'input_shape': input_shape,
            'cnn_filters': cnn_filters,
            'lstm_units': lstm_units,
            'transformer_heads': transformer_heads,
            'transformer_dim': transformer_dim,
            'dense_units': dense_units,
            'dropout_rate': dropout_rate
        }
        
        print(f"âœ… Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        return model
    
    def compile_model(self, model, 
                     optimizer='adam',
                     learning_rate=0.001,
                     loss='mse',
                     metrics=['mae', 'mape']):
        """
        ëª¨ë¸ ì»´íŒŒì¼
        
        Args:
            model (tf.keras.Model): ì»´íŒŒì¼í•  ëª¨ë¸
            optimizer (str): ì˜µí‹°ë§ˆì´ì €
            learning_rate (float): í•™ìŠµë¥ 
            loss (str): ì†ì‹¤ í•¨ìˆ˜
            metrics (list): í‰ê°€ ì§€í‘œ
            
        Returns:
            tf.keras.Model: ì»´íŒŒì¼ëœ ëª¨ë¸
        """
        print(f"âš™ï¸ ëª¨ë¸ ì»´íŒŒì¼ ì¤‘ (optimizer: {optimizer}, lr: {learning_rate})...")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        if optimizer == 'adam':
            opt = Adam(learning_rate=learning_rate)
        elif optimizer == 'rmsprop':
            opt = RMSprop(learning_rate=learning_rate)
        else:
            opt = optimizer
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (Huber loss for robustness)
        if loss == 'huber':
            loss_fn = tf.keras.losses.Huber(delta=1.0)
        else:
            loss_fn = loss
        
        model.compile(
            optimizer=opt,
            loss=loss_fn,
            metrics=metrics
        )
        
        print("âœ… ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ")
        return model
    
    def create_callbacks(self, monitor='val_loss',
                        patience=15,
                        reduce_lr_patience=10,
                        min_lr=1e-6,
                        save_best_only=True,
                        save_path='best_model.h5'):
        """
        ì½œë°± í•¨ìˆ˜ë“¤ ìƒì„±
        
        Args:
            monitor (str): ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ
            patience (int): Early stopping patience
            reduce_lr_patience (int): Learning rate reduction patience
            min_lr (float): ìµœì†Œ í•™ìŠµë¥ 
            save_best_only (bool): ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ì €ì¥
            save_path (str): ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            
        Returns:
            list: ì½œë°± í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        callbacks = []
        
        # Early Stopping
        early_stopping = EarlyStopping(
            monitor=monitor,
            patience=patience,
            restore_best_weights=True,
            verbose=1,
            mode='min' if 'loss' in monitor else 'max'
        )
        callbacks.append(early_stopping)
        
        # Reduce Learning Rate on Plateau
        reduce_lr = ReduceLROnPlateau(
            monitor=monitor,
            factor=0.5,
            patience=reduce_lr_patience,
            min_lr=min_lr,
            verbose=1,
            mode='min' if 'loss' in monitor else 'max'
        )
        callbacks.append(reduce_lr)
        
        # Model Checkpoint
        checkpoint = ModelCheckpoint(
            filepath=save_path,
            monitor=monitor,
            save_best_only=save_best_only,
            save_weights_only=False,
            verbose=1,
            mode='min' if 'loss' in monitor else 'max'
        )
        callbacks.append(checkpoint)
        
        # CSV Logger
        csv_logger = CSVLogger('training_log.csv', append=True)
        callbacks.append(csv_logger)
        
        print(f"âœ… ì½œë°± í•¨ìˆ˜ ìƒì„± ì™„ë£Œ: {len(callbacks)}ê°œ")
        return callbacks
    
    def get_model_summary(self, model):
        """
        ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ì •ë³´ ì¶œë ¥
        
        Args:
            model (tf.keras.Model): ìš”ì•½í•  ëª¨ë¸
        """
        print("="*60)
        print("ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡° ìš”ì•½")
        print("="*60)
        
        model.summary()
        
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"  - ëª¨ë¸ëª…: {model.name}")
        print(f"  - ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        print(f"  - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}")
        print(f"  - ë¹„í•™ìŠµ íŒŒë¼ë¯¸í„°: {sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights]):,}")
        print(f"  - ë ˆì´ì–´ ìˆ˜: {len(model.layers)}")
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        if self.model_config:
            print(f"\nâš™ï¸ ëª¨ë¸ ì„¤ì •:")
            for key, value in self.model_config.items():
                print(f"  - {key}: {value}")
    
    def save_model_architecture(self, model, filepath='model_architecture.png'):
        """
        ëª¨ë¸ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥
        
        Args:
            model (tf.keras.Model): ì €ì¥í•  ëª¨ë¸
            filepath (str): ì €ì¥ ê²½ë¡œ
        """
        try:
            tf.keras.utils.plot_model(
                model,
                to_file=filepath,
                show_shapes=True,
                show_layer_names=True,
                rankdir='TB',
                expand_nested=True,
                dpi=150
            )
            print(f"âœ… ëª¨ë¸ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨ì´ '{filepath}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ graphviz íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: pip install graphviz")

class CustomLosses:
    """ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë“¤"""
    
    @staticmethod
    def weighted_mse(alpha=1.0):
        """
        ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ MSE ì†ì‹¤
        
        Args:
            alpha (float): ê°€ì¤‘ì¹˜ ê°•ë„
            
        Returns:
            function: ì†ì‹¤ í•¨ìˆ˜
        """
        def loss(y_true, y_pred):
            # ë†’ì€ ê°’ì— ë” í° ê°€ì¤‘ì¹˜ ì ìš©
            weights = 1 + alpha * tf.abs(y_true) / tf.reduce_max(tf.abs(y_true))
            mse = tf.square(y_true - y_pred)
            weighted_mse = mse * weights
            return tf.reduce_mean(weighted_mse)
        
        return loss
    
    @staticmethod
    def quantile_loss(quantile=0.5):
        """
        ë¶„ìœ„ìˆ˜ ì†ì‹¤ (Quantile Loss)
        
        Args:
            quantile (float): ë¶„ìœ„ìˆ˜ (0~1)
            
        Returns:
            function: ì†ì‹¤ í•¨ìˆ˜
        """
        def loss(y_true, y_pred):
            error = y_true - y_pred
            return tf.reduce_mean(
                tf.maximum(quantile * error, (quantile - 1) * error)
            )
        
        return loss
    
    @staticmethod
    def focal_mse(gamma=2.0):
        """
        Focal MSE ì†ì‹¤ (ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘)
        
        Args:
            gamma (float): í¬ì»¤ì‹± íŒŒë¼ë¯¸í„°
            
        Returns:
            function: ì†ì‹¤ í•¨ìˆ˜
        """
        def loss(y_true, y_pred):
            mse = tf.square(y_true - y_pred)
            # ì •ê·œí™”ëœ ì˜¤ì°¨
            normalized_error = mse / (tf.reduce_max(mse) + 1e-8)
            # Focal weight
            focal_weight = tf.pow(normalized_error, gamma)
            return tf.reduce_mean(focal_weight * mse)
        
        return loss

class ModelEnsemble:
    """ëª¨ë¸ ì•™ìƒë¸” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = []
        self.weights = []
    
    def add_model(self, model, weight=1.0):
        """
        ì•™ìƒë¸”ì— ëª¨ë¸ ì¶”ê°€
        
        Args:
            model (tf.keras.Model): ì¶”ê°€í•  ëª¨ë¸
            weight (float): ëª¨ë¸ ê°€ì¤‘ì¹˜
        """
        self.models.append(model)
        self.weights.append(weight)
    
    def predict(self, X):
        """
        ì•™ìƒë¸” ì˜ˆì¸¡
        
        Args:
            X (np.array): ì…ë ¥ ë°ì´í„°
            
        Returns:
            np.array: ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡ê°’
        """
        if not self.models:
            raise ValueError("ì•™ìƒë¸”ì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        predictions = []
        total_weight = sum(self.weights)
        
        for model, weight in zip(self.models, self.weights):
            pred = model.predict(X, verbose=0)
            predictions.append(pred * (weight / total_weight))
        
        return np.sum(predictions, axis=0)
    
    def save_ensemble(self, filepath_prefix='ensemble_model'):
        """
        ì•™ìƒë¸” ëª¨ë¸ë“¤ ì €ì¥
        
        Args:
            filepath_prefix (str): íŒŒì¼ëª… ì ‘ë‘ì‚¬
        """
        for i, model in enumerate(self.models):
            filepath = f"{filepath_prefix}_{i}.h5"
            model.save(filepath)
            print(f"ëª¨ë¸ {i} ì €ì¥: {filepath}")
        
        # ê°€ì¤‘ì¹˜ ì •ë³´ ì €ì¥
        weights_info = {
            'weights': self.weights,
            'model_count': len(self.models)
        }
        
        import json
        with open(f"{filepath_prefix}_weights.json", 'w') as f:
            json.dump(weights_info, f)
        
        print(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì €ì¥: {filepath_prefix}_weights.json")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ëª¨ë¸ ë¹Œë” ì´ˆê¸°í™”
    builder = CNNLSTMBuilder()
    
    # ì˜ˆì œ ì…ë ¥ í˜•íƒœ (24 ì‹œê°„ ì‹œí€€ìŠ¤, 20ê°œ íŠ¹ì„±)
    input_shape = (24, 20)
    
    print("ğŸ”¨ ë‹¤ì–‘í•œ CNN-LSTM ëª¨ë¸ êµ¬ì¶• í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # 1. ê¸°ë³¸ CNN-LSTM ëª¨ë¸
    print("\n1ï¸âƒ£ ê¸°ë³¸ CNN-LSTM ëª¨ë¸")
    basic_model = builder.build_basic_cnn_lstm(input_shape)
    basic_model = builder.compile_model(basic_model)
    builder.get_model_summary(basic_model)
    
    # 2. ê³ ê¸‰ CNN-LSTM ëª¨ë¸  
    print("\n2ï¸âƒ£ ê³ ê¸‰ CNN-LSTM ëª¨ë¸")
    advanced_model = builder.build_advanced_cnn_lstm(
        input_shape,
        use_attention=True,
        use_bidirectional=True
    )
    advanced_model = builder.compile_model(advanced_model, loss='huber')
    builder.get_model_summary(advanced_model)
    
    # 3. Transformer-CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ
    print("\n3ï¸âƒ£ Transformer-CNN-LSTM í•˜ì´ë¸Œë¦¬ë“œ")
    hybrid_model = builder.build_transformer_cnn_lstm(input_shape)
    hybrid_model = builder.compile_model(hybrid_model)
    builder.get_model_summary(hybrid_model)
    
    # 4. ì½œë°± í•¨ìˆ˜ ìƒì„±
    print("\n4ï¸âƒ£ ì½œë°± í•¨ìˆ˜ ìƒì„±")
    callbacks = builder.create_callbacks()
    
    # 5. ì•™ìƒë¸” ì˜ˆì œ
    print("\n5ï¸âƒ£ ëª¨ë¸ ì•™ìƒë¸” ì˜ˆì œ")
    ensemble = ModelEnsemble()
    ensemble.add_model(basic_model, weight=0.4)
    ensemble.add_model(advanced_model, weight=0.6)
    
    print("ğŸ‰ ëª¨ë“  ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
