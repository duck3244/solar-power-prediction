#!/usr/bin/env python3
"""
model_trainer.py
CNN-LSTM ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ëª¨ë“ˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.callbacks import History
import warnings

warnings.filterwarnings('ignore')


class ModelTrainer:
    """CNN-LSTM ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self):
        self.model = None
        self.history = None
        self.train_data = None
        self.val_data = None
        self.test_data = None
        self.scalers = None

    def prepare_train_data(self, X, y,
                           train_ratio=0.7,
                           val_ratio=0.15,
                           test_ratio=0.15,
                           shuffle=False,
                           random_state=42):
        """
        í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 

        Args:
            X (np.array): ì…ë ¥ íŠ¹ì„±
            y (np.array): íƒ€ê²Ÿ ë³€ìˆ˜
            train_ratio (float): í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            val_ratio (float): ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            test_ratio (float): í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            shuffle (bool): ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ì‹œê³„ì—´ì˜ ê²½ìš° False ê¶Œì¥)
            random_state (int): ëœë¤ ì‹œë“œ

        Returns:
            tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        print("ğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...")

        # ë¹„ìœ¨ ê²€ì¦
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise ValueError("train_ratio + val_ratio + test_ratio = 1.0ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

        n_samples = len(X)
        train_size = int(n_samples * train_ratio)
        val_size = int(n_samples * val_ratio)

        if shuffle:
            # ëœë¤ ë¶„í•  (ë¹„ì‹œê³„ì—´ ë°ì´í„°ìš©)
            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y, test_size=(1 - train_ratio), random_state=random_state, shuffle=True
            )

            val_test_ratio = val_ratio / (val_ratio + test_ratio)
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=(1 - val_test_ratio), random_state=random_state, shuffle=True
            )
        else:
            # ìˆœì°¨ì  ë¶„í•  (ì‹œê³„ì—´ ë°ì´í„°ìš©)
            X_train = X[:train_size]
            X_val = X[train_size:train_size + val_size]
            X_test = X[train_size + val_size:]

            y_train = y[:train_size]
            y_val = y[train_size:train_size + val_size]
            y_test = y[train_size + val_size:]

        # ë°ì´í„° ì •ë³´ ì €ì¥
        self.train_data = (X_train, y_train)
        self.val_data = (X_val, y_val)
        self.test_data = (X_test, y_test)

        print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"  - í•™ìŠµ: {len(X_train)} ìƒ˜í”Œ")
        print(f"  - ê²€ì¦: {len(X_val)} ìƒ˜í”Œ")
        print(f"  - í…ŒìŠ¤íŠ¸: {len(X_test)} ìƒ˜í”Œ")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def train_model(self, model, X_train, y_train, X_val, y_val,
                    epochs=100, batch_size=32, callbacks=None,
                    verbose=1, validation_freq=1):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            model (tf.keras.Model): í•™ìŠµí•  ëª¨ë¸
            X_train (np.array): í•™ìŠµ ì…ë ¥ ë°ì´í„°
            y_train (np.array): í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„°
            X_val (np.array): ê²€ì¦ ì…ë ¥ ë°ì´í„°
            y_val (np.array): ê²€ì¦ íƒ€ê²Ÿ ë°ì´í„°
            epochs (int): ì—í¬í¬ ìˆ˜
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            callbacks (list): ì½œë°± í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
            verbose (int): ì¶œë ¥ ë ˆë²¨
            validation_freq (int): ê²€ì¦ ì£¼ê¸°

        Returns:
            tf.keras.callbacks.History: í•™ìŠµ ì´ë ¥
        """
        print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print("=" * 60)

        self.model = model

        # í•™ìŠµ ì‹¤í–‰
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=verbose,
            validation_freq=validation_freq,
            shuffle=True
        )

        self.history = history

        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        return history

    def evaluate_model(self, model, X_test, y_test, scalers=None, verbose=1):
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

        Args:
            model (tf.keras.Model): í‰ê°€í•  ëª¨ë¸
            X_test (np.array): í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„°
            y_test (np.array): í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°
            scalers (dict): ìŠ¤ì¼€ì¼ëŸ¬ ë”•ì…”ë„ˆë¦¬
            verbose (int): ì¶œë ¥ ë ˆë²¨

        Returns:
            tuple: (metrics_dict, y_true, y_pred)
        """
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")

        # ì˜ˆì¸¡
        y_pred_scaled = model.predict(X_test, verbose=verbose)

        # ì—­ì •ê·œí™” (ìŠ¤ì¼€ì¼ëŸ¬ê°€ ìˆëŠ” ê²½ìš°)
        if scalers and 'target_scaler' in scalers:
            y_pred = scalers['target_scaler'].inverse_transform(y_pred_scaled)
            y_true = scalers['target_scaler'].inverse_transform(y_test)
        else:
            y_pred = y_pred_scaled
            y_true = y_test

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        # MAPE ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        mask = y_true != 0
        if mask.any():
            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        else:
            mape = np.inf

        # ë°©í–¥ì„± ì •í™•ë„ (Direction Accuracy)
        if len(y_true) > 1:
            y_true_diff = np.diff(y_true.flatten())
            y_pred_diff = np.diff(y_pred.flatten())
            direction_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff)) * 100
        else:
            direction_accuracy = 0

        metrics = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'MAPE': mape,
            'Direction_Accuracy': direction_accuracy
        }

        # ê²°ê³¼ ì¶œë ¥
        print("=" * 60)
        print("ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
        print("=" * 60)
        for metric_name, value in metrics.items():
            if metric_name == 'Direction_Accuracy':
                print(f"{metric_name:>18}: {value:.2f}%")
            elif metric_name in ['MAPE']:
                print(f"{metric_name:>18}: {value:.2f}%")
            else:
                print(f"{metric_name:>18}: {value:.4f}")
        print("=" * 60)

        return metrics, y_true, y_pred

    def cross_validate(self, model_builder, X, y, cv_folds=5, **model_params):
        """
        ì‹œê³„ì—´ êµì°¨ ê²€ì¦

        Args:
            model_builder (function): ëª¨ë¸ ìƒì„± í•¨ìˆ˜
            X (np.array): ì…ë ¥ ë°ì´í„°
            y (np.array): íƒ€ê²Ÿ ë°ì´í„°
            cv_folds (int): êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜
            **model_params: ëª¨ë¸ ìƒì„± íŒŒë¼ë¯¸í„°

        Returns:
            dict: êµì°¨ ê²€ì¦ ê²°ê³¼
        """
        print(f"ğŸ”„ {cv_folds}-í´ë“œ ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ì‹œì‘...")

        tscv = TimeSeriesSplit(n_splits=cv_folds)
        cv_scores = {
            'RMSE': [], 'MAE': [], 'RÂ²': [], 'MAPE': []
        }

        fold = 1
        for train_idx, test_idx in tscv.split(X):
            print(f"\nğŸ“ Fold {fold}/{cv_folds}")

            X_train_cv, X_test_cv = X[train_idx], X[test_idx]
            y_train_cv, y_test_cv = y[train_idx], y[test_idx]

            # ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
            model = model_builder(input_shape=X.shape[1:], **model_params)

            # í•™ìŠµ (ì ì€ ì—í¬í¬ë¡œ ë¹ ë¥´ê²Œ)
            model.fit(
                X_train_cv, y_train_cv,
                epochs=20,
                batch_size=32,
                verbose=0,
                validation_split=0.2
            )

            # í‰ê°€
            y_pred = model.predict(X_test_cv, verbose=0)

            rmse = np.sqrt(mean_squared_error(y_test_cv, y_pred))
            mae = mean_absolute_error(y_test_cv, y_pred)
            r2 = r2_score(y_test_cv, y_pred)

            # MAPE ê³„ì‚°
            mask = y_test_cv != 0
            mape = np.mean(np.abs((y_test_cv[mask] - y_pred[mask]) / y_test_cv[mask])) * 100 if mask.any() else np.inf

            cv_scores['RMSE'].append(rmse)
            cv_scores['MAE'].append(mae)
            cv_scores['RÂ²'].append(r2)
            cv_scores['MAPE'].append(mape)

            print(f"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}")

            fold += 1

        # êµì°¨ ê²€ì¦ ê²°ê³¼ ìš”ì•½
        cv_results = {}
        for metric in cv_scores.keys():
            scores = cv_scores[metric]
            cv_results[metric] = {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'scores': scores
            }

        print("\n" + "=" * 60)
        print("ğŸ“Š êµì°¨ ê²€ì¦ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        for metric, result in cv_results.items():
            print(f"{metric:>6}: {result['mean']:.4f} Â± {result['std']:.4f}")
        print("=" * 60)

        return cv_results

    def plot_training_history(self, history=None, figsize=(15, 10)):
        """
        í•™ìŠµ ê³¼ì • ì‹œê°í™”

        Args:
            history (tf.keras.callbacks.History): í•™ìŠµ ì´ë ¥ (Noneì´ë©´ self.history ì‚¬ìš©)
            figsize (tuple): ê·¸ë˜í”„ í¬ê¸°
        """
        if history is None:
            history = self.history

        if history is None:
            print("âŒ í•™ìŠµ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        history_dict = history.history
        epochs = range(1, len(history_dict['loss']) + 1)

        # ê·¸ë˜í”„ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('CNN-LSTM model training process', fontsize=16, fontweight='bold')

        # Loss
        axes[0, 0].plot(epochs, history_dict['loss'], 'b-', label='Training Loss', linewidth=2)
        if 'val_loss' in history_dict:
            axes[0, 0].plot(epochs, history_dict['val_loss'], 'r-', label='Validation Loss', linewidth=2)
        axes[0, 0].set_title('ğŸ“‰ Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # MAE
        if 'mae' in history_dict:
            axes[0, 1].plot(epochs, history_dict['mae'], 'g-', label='Training MAE', linewidth=2)
            if 'val_mae' in history_dict:
                axes[0, 1].plot(epochs, history_dict['val_mae'], 'orange', label='Validation MAE', linewidth=2)
            axes[0, 1].set_title('Mean Absolute Error')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('MAE')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # MAPE
        if 'mape' in history_dict:
            axes[1, 0].plot(epochs, history_dict['mape'], 'purple', label='Training MAPE', linewidth=2)
            if 'val_mape' in history_dict:
                axes[1, 0].plot(epochs, history_dict['val_mape'], 'brown', label='Validation MAPE', linewidth=2)
            axes[1, 0].set_title('Mean Absolute Percentage Error')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('MAPE (%)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Learning Rate (if available)
        if 'lr' in history_dict:
            axes[1, 1].plot(epochs, history_dict['lr'], 'red', linewidth=2)
            axes[1, 1].set_title('Learning Rate')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Learning Rate')
            axes[1, 1].set_yscale('log')
            axes[1, 1].grid(True, alpha=0.3)
        else:
            # í•™ìŠµ ì‹œê°„ ë˜ëŠ” ê¸°íƒ€ ì •ë³´ í‘œì‹œ
            axes[1, 1].text(0.5, 0.5, 'Learning Rate\nScheduling Info\nNot Available',
                            ha='center', va='center', transform=axes[1, 1].transAxes,
                            fontsize=12, color='gray')
            axes[1, 1].set_xticks([])
            axes[1, 1].set_yticks([])

        plt.tight_layout()
        plt.show()

    def plot_predictions(self, y_true, y_pred,
                         sample_range=None, figsize=(16, 12)):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”

        Args:
            y_true (np.array): ì‹¤ì œê°’
            y_pred (np.array): ì˜ˆì¸¡ê°’
            sample_range (tuple): í‘œì‹œí•  ìƒ˜í”Œ ë²”ìœ„ (start, end)
            figsize (tuple): ê·¸ë˜í”„ í¬ê¸°
        """
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle('CNN-LSTM power demand prediction results', fontsize=16, fontweight='bold')

        y_true_flat = y_true.flatten()
        y_pred_flat = y_pred.flatten()

        # ìƒ˜í”Œ ë²”ìœ„ ì„¤ì •
        if sample_range is None:
            start, end = 0, min(100, len(y_true_flat))
        else:
            start, end = sample_range
            end = min(end, len(y_true_flat))

        # 1. ì‹œê³„ì—´ ë¹„êµ
        indices = range(start, end)
        axes[0, 0].plot(indices, y_true_flat[start:end], 'b-',
                        label='Actual value', linewidth=2, alpha=0.8)
        axes[0, 0].plot(indices, y_pred_flat[start:end], 'r-',
                        label='Predicted value', linewidth=2, alpha=0.8)
        axes[0, 0].set_title('Time series forecast comparison')
        axes[0, 0].set_xlabel('Time (Hours)')
        axes[0, 0].set_ylabel('Electricity demand (kW)')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. ì‚°ì ë„
        axes[0, 1].scatter(y_true_flat, y_pred_flat, alpha=0.6, color='green', s=20)
        min_val = min(y_true_flat.min(), y_pred_flat.min())
        max_val = max(y_true_flat.max(), y_pred_flat.max())
        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
        axes[0, 1].set_title('Actual vs. predicted values')
        axes[0, 1].set_xlabel('Actual value (kW)')
        axes[0, 1].set_ylabel('Predicted value (kW)')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ì”ì°¨ ë¶„ì„
        residuals = y_true_flat - y_pred_flat
        axes[1, 0].scatter(y_pred_flat, residuals, alpha=0.6, color='orange', s=20)
        axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
        axes[1, 0].set_title('Residual analysis')
        axes[1, 0].set_xlabel('Actual value (kW)')
        axes[1, 0].set_ylabel('Predicted value (kW)')
        axes[1, 0].grid(True, alpha=0.3)

        # 4. ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
        axes[1, 1].hist(residuals, bins=30, alpha=0.7, color='purple', edgecolor='black')
        axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)
        axes[1, 1].axvline(x=np.mean(residuals), color='green', linestyle='-', linewidth=2,
                           label=f'Average: {np.mean(residuals):.2f}')
        axes[1, 1].set_title('Residual distribution')
        axes[1, 1].set_xlabel('Residual (kW)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # ì¶”ê°€ í†µê³„ ì •ë³´ ì¶œë ¥
        print("\nResidual statistics:")
        print(f"  - Average: {np.mean(residuals):.4f}")
        print(f"  - Standard deviation: {np.std(residuals):.4f}")
        print(f"  - Minimum value: {np.min(residuals):.4f}")
        print(f"  - Maximum value: {np.max(residuals):.4f}")

    def plot_feature_importance(self, model, feature_names, top_n=20):
        """
        íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” (ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê·¼ì‚¬)

        Args:
            model (tf.keras.Model): í•™ìŠµëœ ëª¨ë¸
            feature_names (list): íŠ¹ì„±ëª… ë¦¬ìŠ¤íŠ¸
            top_n (int): í‘œì‹œí•  ìƒìœ„ íŠ¹ì„± ê°œìˆ˜
        """
        try:
            # ì²« ë²ˆì§¸ Dense ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ íŠ¹ì„± ì¤‘ìš”ë„ë¡œ ê·¼ì‚¬
            dense_layers = [layer for layer in model.layers if 'dense' in layer.name.lower()]

            if not dense_layers:
                print("âŒ Dense ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ì²« ë²ˆì§¸ ìœµí•© Dense ë ˆì´ì–´ ì°¾ê¸°
            fusion_layer = None
            for layer in model.layers:
                if 'fusion' in layer.name or 'concatenate' in layer.name:
                    # Concatenate ë‹¤ìŒì˜ Dense ë ˆì´ì–´ ì°¾ê¸°
                    layer_idx = model.layers.index(layer)
                    for i in range(layer_idx + 1, len(model.layers)):
                        if isinstance(model.layers[i], tf.keras.layers.Dense):
                            fusion_layer = model.layers[i]
                            break
                    break

            if fusion_layer is None:
                fusion_layer = dense_layers[0]

            weights = fusion_layer.get_weights()[0]  # [input_dim, output_dim]

            # ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í‰ê· ì„ ì¤‘ìš”ë„ë¡œ ì‚¬ìš©
            if len(weights.shape) == 2:
                importance_scores = np.mean(np.abs(weights), axis=1)
            else:
                importance_scores = np.abs(weights)

            # íŠ¹ì„±ëª…ê³¼ ë§¤ì¹­ (ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ì •)
            if len(importance_scores) != len(feature_names):
                print(f"âš ï¸ ê°€ì¤‘ì¹˜ ì°¨ì›({len(importance_scores)})ê³¼ íŠ¹ì„± ìˆ˜({len(feature_names)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
                # CNN-LSTM ìœµí•© í›„ì˜ ì°¨ì›ì´ë¯€ë¡œ íŠ¹ì„±ëª… ë§¤ì¹­ ë¶ˆê°€
                feature_names = [f'Feature_{i}' for i in range(len(importance_scores))]

            # ì¤‘ìš”ë„ ì •ë ¬
            feature_importance = pd.Series(importance_scores, index=feature_names)
            feature_importance = feature_importance.sort_values(ascending=True)

            # ìƒìœ„ Nê°œ íŠ¹ì„±ë§Œ í‘œì‹œ
            top_features = feature_importance.tail(top_n)

            # ì‹œê°í™”
            plt.figure(figsize=(12, 8))
            colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
            bars = plt.barh(range(len(top_features)), top_features.values, color=colors)

            plt.yticks(range(len(top_features)), top_features.index)
            plt.xlabel('Weighted importance (absolute mean)')
            plt.title(f'Top {top_n} feature importance (weighted approximation)')
            plt.grid(True, alpha=0.3)

            # ê°’ í‘œì‹œ
            for i, (bar, value) in enumerate(zip(bars, top_features.values)):
                plt.text(value + max(top_features.values) * 0.01, bar.get_y() + bar.get_height() / 2,
                         f'{value:.4f}', va='center', fontsize=9)

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"âŒ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")

    def save_results(self, metrics, y_true, y_pred, filepath_prefix='model_results'):
        """
        ê²°ê³¼ ì €ì¥

        Args:
            metrics (dict): í‰ê°€ ì§€í‘œ
            y_true (np.array): ì‹¤ì œê°’
            y_pred (np.array): ì˜ˆì¸¡ê°’
            filepath_prefix (str): ì €ì¥ íŒŒì¼ëª… ì ‘ë‘ì‚¬
        """
        import json
        from datetime import datetime

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. í‰ê°€ ì§€í‘œ ì €ì¥
        metrics_file = f"{filepath_prefix}_metrics_{timestamp}.json"
        with open(metrics_file, 'w') as f:
            # numpy íƒ€ì…ì„ ì¼ë°˜ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
            metrics_serializable = {}
            for key, value in metrics.items():
                if isinstance(value, np.ndarray):
                    metrics_serializable[key] = value.tolist()
                elif isinstance(value, (np.int64, np.int32, np.float64, np.float32)):
                    metrics_serializable[key] = float(value)
                else:
                    metrics_serializable[key] = value

            json.dump(metrics_serializable, f, indent=2)

        print(f"âœ… í‰ê°€ ì§€í‘œ ì €ì¥: {metrics_file}")

        # 2. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        results_df = pd.DataFrame({
            'actual': y_true.flatten(),
            'predicted': y_pred.flatten(),
            'residual': y_true.flatten() - y_pred.flatten()
        })

        results_file = f"{filepath_prefix}_predictions_{timestamp}.csv"
        results_df.to_csv(results_file, index=False)
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {results_file}")

        # 3. í•™ìŠµ ì´ë ¥ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        if self.history is not None:
            history_df = pd.DataFrame(self.history.history)
            history_file = f"{filepath_prefix}_history_{timestamp}.csv"
            history_df.to_csv(history_file, index=False)
            print(f"âœ… í•™ìŠµ ì´ë ¥ ì €ì¥: {history_file}")
        else:
            history_file = None

        return {
            'metrics_file': metrics_file,
            'predictions_file': results_file,
            'history_file': history_file
        }

    def load_and_resume_training(self, model_path, X_train, y_train, X_val, y_val,
                                 additional_epochs=50, **training_params):
        """
        ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í•™ìŠµ ì¬ê°œ

        Args:
            model_path (str): ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            X_train, y_train: í•™ìŠµ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            additional_epochs (int): ì¶”ê°€ í•™ìŠµ ì—í¬í¬
            **training_params: ê¸°íƒ€ í•™ìŠµ íŒŒë¼ë¯¸í„°

        Returns:
            tf.keras.callbacks.History: ì¶”ê°€ í•™ìŠµ ì´ë ¥
        """
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")

        try:
            self.model = tf.keras.models.load_model(model_path)
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # í•™ìŠµ ì¬ê°œ
            print(f"ğŸ”„ {additional_epochs} ì—í¬í¬ ì¶”ê°€ í•™ìŠµ ì‹œì‘...")

            history = self.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=additional_epochs,
                **training_params
            )

            # ê¸°ì¡´ ì´ë ¥ê³¼ ë³‘í•© (ìˆëŠ” ê²½ìš°)
            if self.history is not None:
                for key in history.history.keys():
                    if key in self.history.history:
                        self.history.history[key].extend(history.history[key])
                    else:
                        self.history.history[key] = history.history[key]
            else:
                self.history = history

            print("âœ… ì¶”ê°€ í•™ìŠµ ì™„ë£Œ")
            return history

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ë° í•™ìŠµ ì¬ê°œ ì‹¤íŒ¨: {e}")
            raise


class HyperparameterTuner:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í´ë˜ìŠ¤"""

    def __init__(self, model_builder):
        self.model_builder = model_builder
        self.best_params = None
        self.best_score = float('inf')
        self.tuning_results = []

    def grid_search(self, X_train, y_train, X_val, y_val, param_grid,
                    scoring='rmse', max_epochs=30):
        """
        ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

        Args:
            X_train, y_train: í•™ìŠµ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            param_grid (dict): íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
            scoring (str): í‰ê°€ ì§€í‘œ
            max_epochs (int): ìµœëŒ€ ì—í¬í¬ (ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´)

        Returns:
            dict: ìµœì  íŒŒë¼ë¯¸í„°ì™€ ê²°ê³¼
        """
        print("ğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
        print(f"íƒìƒ‰í•  ì¡°í•© ìˆ˜: {np.prod([len(v) for v in param_grid.values()])}")

        from itertools import product

        # ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•© ìƒì„±
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())

        combination_count = 0
        total_combinations = np.prod([len(v) for v in param_values])

        for combination in product(*param_values):
            combination_count += 1
            params = dict(zip(param_names, combination))

            print(f"\nğŸ§ª ì¡°í•© {combination_count}/{total_combinations}: {params}")

            try:
                # ëª¨ë¸ ìƒì„±
                model = self.model_builder(input_shape=X_train.shape[1:], **params)

                # ê°„ë‹¨í•œ ì»´íŒŒì¼ ë° í•™ìŠµ
                model.compile(optimizer='adam', loss='mse', metrics=['mae'])

                history = model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=max_epochs,
                    batch_size=32,
                    verbose=0
                )

                # ì„±ëŠ¥ í‰ê°€
                y_pred = model.predict(X_val, verbose=0)

                if scoring == 'rmse':
                    score = np.sqrt(mean_squared_error(y_val, y_pred))
                elif scoring == 'mae':
                    score = mean_absolute_error(y_val, y_pred)
                elif scoring == 'r2':
                    score = -r2_score(y_val, y_pred)  # ìŒìˆ˜ë¡œ ë³€í™˜ (ìµœì†Œí™” ëª©í‘œ)
                else:
                    score = np.sqrt(mean_squared_error(y_val, y_pred))

                # ê²°ê³¼ ì €ì¥
                result = {
                    'params': params,
                    'score': score,
                    'final_val_loss': history.history['val_loss'][-1]
                }
                self.tuning_results.append(result)

                print(f"  Score ({scoring}): {score:.4f}")

                # ìµœì  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
                if score < self.best_score:
                    self.best_score = score
                    self.best_params = params
                    print(f"  ğŸ† ìƒˆë¡œìš´ ìµœì  íŒŒë¼ë¯¸í„°! Score: {score:.4f}")

            except Exception as e:
                print(f"  âŒ ì¡°í•© ì‹¤íŒ¨: {e}")
                continue

        print("\n" + "=" * 60)
        print("ğŸ† í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼")
        print("=" * 60)
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params}")
        print(f"ìµœì  ì ìˆ˜ ({scoring}): {self.best_score:.4f}")

        # ìƒìœ„ 5ê°œ ê²°ê³¼ ì¶œë ¥
        sorted_results = sorted(self.tuning_results, key=lambda x: x['score'])[:5]
        print(f"\nğŸ“Š ìƒìœ„ 5ê°œ ê²°ê³¼:")
        for i, result in enumerate(sorted_results, 1):
            print(f"  {i}. Score: {result['score']:.4f}, Params: {result['params']}")

        return {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'all_results': self.tuning_results
        }


# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    from cnn_lstm_model import CNNLSTMBuilder

    print("ğŸ§ª ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì˜ˆì œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples, seq_length, n_features = 1000, 24, 20
    X = np.random.randn(n_samples, seq_length, n_features)
    y = np.random.randn(n_samples, 1)

    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = ModelTrainer()

    # ë°ì´í„° ë¶„í• 
    X_train, X_val, X_test, y_train, y_val, y_test = trainer.prepare_train_data(X, y)

    # ëª¨ë¸ êµ¬ì¶•
    builder = CNNLSTMBuilder()
    model = builder.build_basic_cnn_lstm(input_shape=X.shape[1:])
    model = builder.compile_model(model)

    # ì½œë°± ìƒì„±
    callbacks = builder.create_callbacks()

    # ëª¨ë¸ í•™ìŠµ
    history = trainer.train_model(
        model, X_train, y_train, X_val, y_val,
        epochs=20, batch_size=32, callbacks=callbacks
    )

    # ëª¨ë¸ í‰ê°€
    metrics, y_true, y_pred = trainer.evaluate_model(model, X_test, y_test)

    # ê²°ê³¼ ì‹œê°í™”
    trainer.plot_training_history()
    trainer.plot_predictions(y_true, y_pred)

    # ê²°ê³¼ ì €ì¥
    saved_files = trainer.save_results(metrics, y_true, y_pred)

    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")